{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0: 51069\n",
      "Split 1: 54059\n",
      "Split 2: 72263\n",
      "Split 3: 53178\n",
      "Split 4: 42936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codegraphs/diversevul/v2_undirected_withdegreecount/5ac224602f1d603aac5eaa72e1760d3e33a26f0a_120848610895421645248357269512901044407_0.cpg.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2222/2222 [00:00<00:00, 2360.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from models.BareboneGCN import BareboneGCN\n",
    "from CodeGraphDataset_diversevul_erik_preprocessed import CodeGraphDataset_InMemory\n",
    "dataset4 = CodeGraphDataset_InMemory(pt_folder='codegraphs/diversevul/v2_undirected_withdegreecount', split='train', \n",
    "                                          cross_val_valfold_idx=0, \n",
    "                                          is_cross_val=True, cross_val_train_fraction=0.01, DS_type='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GraphGLOW with DENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    sample = dataset4[i]\n",
    "    i += 1   \n",
    "    if sample.x.shape[0]>910 and sample.x.shape[0]<1010:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = sample.edge_index\n",
    "# make a dense pytorch matrix out of it\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "dense = to_dense_adj(edge_index).squeeze(0)\n",
    "features = sample.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([911, 911])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2222"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "# import linear from torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, Linear\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "def squareroot_degree_inverse_undirected(adj):\n",
    "    row_sum = torch.sum(adj, dim=1)\n",
    "    row_sum = row_sum.pow(-0.5)\n",
    "    D_12 = torch.eye(row_sum.shape[0]).to(row_sum.device) * row_sum\n",
    "    return D_12\n",
    "\n",
    "\n",
    "class DenseGCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DenseGCNConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.matmul(adj, x)\n",
    "        return x + self.bias\n",
    "\n",
    "\n",
    "class BareboneDenseGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, dropout, readout_pooling,in_channels=162, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.pooling = readout_pooling\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        self.initial_layer = DenseGCNConv(in_channels, hidden_channels)\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(num_layers-1):\n",
    "            if i == 0:\n",
    "                self.layers.append(DenseGCNConv(in_channels, hidden_channels))\n",
    "            else:\n",
    "                self.layers.append(DenseGCNConv(hidden_channels, hidden_channels))\n",
    "            \n",
    "        # self.classification_layer = Linear(hidden_channels, 1)\n",
    "        \n",
    "        \n",
    "    def forward_initial_layer(self, x, normalized_adj):\n",
    "        X = self.initial_layer(x, normalized_adj)\n",
    "        X = F.dropout(X, training=self.training, p=self.dropout) # droput before relu faster\n",
    "        X = F.relu(X)\n",
    "        return X\n",
    "\n",
    "    def forward(self, X, normalized_adj, initial_normalized_adj,GSL_skip_ratio):\n",
    "        if GSL_skip_ratio is not None and GSL_skip_ratio>0:  # add skip connections\n",
    "            for layer in self.layers:\n",
    "                X_skip = layer(X, initial_normalized_adj)\n",
    "                X_nonskip = layer(X, normalized_adj)\n",
    "                X = F.relu(GSL_skip_ratio*X_skip + (1-GSL_skip_ratio)*X_nonskip)\n",
    "                X = F.dropout(X, training=self.training, p=self.dropout)\n",
    "        else:   \n",
    "            for layer in self.layers:\n",
    "                X = layer(X, normalized_adj)\n",
    "                X = F.dropout(X, training=self.training, p=self.dropout)\n",
    "                X = F.relu(X)\n",
    "            \n",
    "        # logits = self.classification_layer(X)\n",
    "\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureLearner(torch.nn.Module):\n",
    "    def __init__(self, num_heads, input_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.weight_tensor = torch.nn.Parameter(torch.Tensor(num_heads, input_channels)).unsqueeze(1)\n",
    "        self.weight_tensor = torch.nn.Parameter(torch.nn.init.xavier_uniform_(self.weight_tensor))\n",
    "        self.attention_threshold_epsilon = 0.3\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.unsqueeze(0) \n",
    "        \n",
    "        # hadamard product by broadcasting\n",
    "        # (h, 1, dim) * (1, n, dim) -> (h, n, dim)\n",
    "        X = torch.multiply(self.weight_tensor, X)\n",
    "        Xnorm = F.normalize(X, p=2, dim=-1)\n",
    "        attention = torch.bmm(Xnorm, Xnorm.transpose(1,2).detach())\n",
    "        attention = torch.mean(attention, dim=0)\n",
    "        attention[attention > self.attention_threshold_epsilon] = 0\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 128\n",
    "model = BareboneDenseGCN(hidden_channels=hidden_channels, num_layers=2, dropout=0.5, readout_pooling='mean').cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GCN import GCN\n",
    "edgeindex_model = GCN(hidden_channels=hidden_channels, num_layers=2, dropout=0.5, readout_pooling='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[911, 162], edge_index=[2, 3490], y=[1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([911, 911])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "structureLearner = StructureLearner(num_heads=4, input_channels=hidden_channels).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.BinaryClassifierReadOutLayer import BinaryClassifierReadOutLayer\n",
    "head = BinaryClassifierReadOutLayer(hidden_channels=hidden_channels, num_layers=1, pooling='sum').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape\n",
    "batch = torch.zeros(sample.x.shape[0]).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "batch_size = 8\n",
    "iterations = 5\n",
    "model.train()\n",
    "edgeindex_model.train()\n",
    "y = torch.tensor([[1.0]]).cuda()\n",
    "optimizer1 = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer2 = torch.optim.Adam(edgeindex_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 s ± 67.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# run model\n",
    "\n",
    "for i in range(num_samples):\n",
    "    \n",
    "    dense = to_dense_adj(edge_index).squeeze(0)\n",
    "    normalized_dense_initial = squareroot_degree_inverse_undirected(dense)\n",
    "    features = sample.x\n",
    "    h = model.forward_initial_layer(features, normalized_dense_initial)\n",
    "    for iter in range(iterations):\n",
    "        adj = structureLearner(h)\n",
    "        normalized_dense = squareroot_degree_inverse_undirected(adj)\n",
    "        h = model(features, normalized_dense, normalized_dense_initial, GSL_skip_ratio=0.3) # in graphglow they use same h as the one from the initial layer\n",
    "    \n",
    "    logits = head(h, batch)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "    \n",
    "    \n",
    "    if i % batch_size == 0 and i > 0:\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "    else:\n",
    "        loss.backward(retain_graph=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GraphStructureLearner import GraphStructureLearner\n",
    "sparse_gsl = GraphStructureLearner(num_heads=4, input_channels=hidden_channels, attention_threshold_epsilon=0.3).cuda()\n",
    "has_converged = torch.tensor([False]).cuda()\n",
    "\n",
    "from models.BinaryClassifierReadOutLayer import BinaryClassifierReadOutLayer\n",
    "head2 = BinaryClassifierReadOutLayer(hidden_channels=hidden_channels, num_layers=1, pooling='sum').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m# run model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfor i in range(num_samples):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    features = sample.x\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    h = edgeindex_model.initial_layer(features, sample.edge_index)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    for iter in range(iterations):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        edge_index, edge_weights = sparse_gsl(h, batch, has_converged) # note that this uses the minibatch implementation (iterate over graphs in minibatch seperately), I checked that most time consuming is the bmm and the torch.argwhere, therefore the batch overhead should not be significant\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        h, placeholder = edgeindex_model(features, edge_index, edge_weights, sample.edge_index, GSL_skip_ratio=0.3)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    logits = head(h, batch)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    loss = F.binary_cross_entropy_with_logits(logits, y)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    if i \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m batch_size == 0 and i > 0:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        loss.backward()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        optimizer2.step()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        optimizer2.zero_grad()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    else:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        loss.backward(retain_graph=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/site-packages/IPython/core/magics/execution.py:1189\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1187\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1191\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/timeit.py:208\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    206\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 208\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:18\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pyg/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# run model\n",
    "\n",
    "for i in range(num_samples):\n",
    "    features = sample.x\n",
    "    h = edgeindex_model.initial_layer(features, sample.edge_index)\n",
    "    for iter in range(iterations):\n",
    "        edge_index, edge_weights = sparse_gsl(h, batch, has_converged) # note that this uses the minibatch implementation (iterate over graphs in minibatch seperately), I checked that most time consuming is the bmm and the torch.argwhere, therefore the batch overhead should not be significant\n",
    "        h, placeholder = edgeindex_model(features, edge_index, edge_weights, sample.edge_index, GSL_skip_ratio=0.3)\n",
    "    \n",
    "    logits = head(h, batch)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "    \n",
    "    if i % batch_size == 0 and i > 0:\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "    else:\n",
    "        loss.backward(retain_graph=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3364, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
